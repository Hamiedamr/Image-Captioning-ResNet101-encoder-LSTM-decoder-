{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_resnet(vocab_size, max_length):\n",
    "    \n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(512, activation='relu')(fe1)\n",
    "\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 512, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(512)(se2)\n",
    "\n",
    "    # decoder model\n",
    "    decoder1 = tf.keras.layers.add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=4e-4)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    \n",
    "    # summarize model\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photo_identifiers(filename):\n",
    "    \n",
    "    # Loading the file containing the list of photo identifier\n",
    "    file = load_file(filename)\n",
    "    \n",
    "    # Creating a list for storing the identifiers\n",
    "    photos = list()\n",
    "    \n",
    "    # Traversing the file one line at a time\n",
    "    for line in file.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        \n",
    "        # Image name contains the extension as well but we need just the name\n",
    "        identifier = line.split('.')[0]\n",
    "        \n",
    "        # Adding it to the list of photos\n",
    "        photos.append(identifier)\n",
    "        \n",
    "    # Returning the set of photos created\n",
    "    return set(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, photos):\n",
    "    \n",
    "    #loading the cleaned description file\n",
    "    file = load_file(filename)\n",
    "    \n",
    "    #creating a dictionary of descripitions for storing the photo to description mapping of train images\n",
    "    descriptions = dict()\n",
    "    \n",
    "    #traversing the file line by line\n",
    "    for line in file.split('\\n'):\n",
    "        # splitting the line at white spaces\n",
    "        words = line.split()\n",
    "        \n",
    "        # the first word will be the image name and the rest will be the description of that particular image\n",
    "        image_id, image_description = words[0], words[1:]\n",
    "        \n",
    "        # we want to load only those description which corresponds to the set of photos we provided as argument\n",
    "        if image_id in photos:\n",
    "            #creating list of description if needed\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            \n",
    "            #the model we will develop will generate a caption given a photo, \n",
    "            #and the caption will be generated one word at a time. \n",
    "            #The sequence of previously generated words will be provided as input. \n",
    "            #Therefore, we will need a ‘first word’ to kick-off the generation process \n",
    "            #and a ‘last word‘ to signal the end of the caption.\n",
    "            #we will use 'startseq' and 'endseq' for this purpose\n",
    "            #also we have to convert image description back to string\n",
    "            \n",
    "            desc = 'startseq ' + ' '.join(image_description) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "            \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the photo features created using the VGG16 model\n",
    "def load_photo_features(filename, photos):\n",
    "    \n",
    "    #this will load the entire features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    \n",
    "    #we are interested in loading the features of the required photos only\n",
    "    features = {k: all_features[k] for k in photos}\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    file = open(filename, 'r',encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './datasets/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_photo_identifiers(filename)\n",
    "train_descriptions = load_clean_descriptions('./datasets/descriptions.txt', train)\n",
    "tokenizer = load(open('./flickr30k/tokenizer30k.pkl', 'rb'))\n",
    "train_features = load_photo_features('./features_resnet.pkl', train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "lines = to_lines(train_descriptions)\n",
    "max_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length,bs=32):\n",
    "    while 1:\n",
    "        for key, description_list in descriptions.items():\n",
    "            #retrieve photo features\n",
    "            photo = photos[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, photo)\n",
    "            yield [input_image,input_sequence],output_word\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model_resnet(vocab_size, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(train_descriptions)\n",
    "generator = data_generator(train_descriptions, train_features, tokenizer, 16)\n",
    "filename = './saved8k/flick'\n",
    "model.load_weights(filename)\n",
    "history = model.fit(generator, epochs=5000,verbose=0,steps_per_epoch=16,callbacks = [ModelCheckpoint(filepath='final_model.h5',\n",
    "        save_weights_only=True,                                                                                 \n",
    "        monitor='loss',\n",
    "        save_best_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history.epoch,history.history['accuracy'])\n",
    "plt.plot(history.epoch,history.history['loss'],history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './saved8k/flick'\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_resnet(filename):\n",
    "    model = ResNet101(weights=\"imagenet\")\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D://GP//Image_captioning//datasets/Flicker8k_Dataset/700884207_d3ec546494.jpg\"\n",
    "photo_resnet = extract_features_resnet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_resnet = generate_desc(model, tokenizer, photo_resnet, 16)\n",
    "plt.imshow(plt.imread(path))\n",
    "print(\"ResNet101 output: \" + description_resnet.strip('startseqendseq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
